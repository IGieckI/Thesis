{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    judg_id                                           epigrafe  \\\n",
      "0  161/1982  Nei giudizi di legittimità costituzionale dell...   \n",
      "1    9/1965  Nei giudizi riuniti di legittimità costituzion...   \n",
      "2    2/2008  Nei giudizi di legittimità costituzionale dell...   \n",
      "3  176/1982  Nei giudizi riuniti di legittimità costituzion...   \n",
      "4   73/1990  Nel giudizio di legittimità costituzionale del...   \n",
      "\n",
      "                                   ritenuto_in_fatto  \\\n",
      "0  . 1.1. - Con ricorso, notificato il 21 gennaio...   \n",
      "1  . 1. - Nel corso di un procedimento penale a c...   \n",
      "2  1. – Il Tribunale di Perugia in composizione m...   \n",
      "3  . 1. - Con tre ordinanze sostanzialmente coinc...   \n",
      "4  . 1. - Nel corso di un giudizio amministrativo...   \n",
      "\n",
      "                              considerato_in_diritto  \\\n",
      "0  . 16.1. - Nelle motivazioni delle ordinanze de...   \n",
      "1  . 1. - Tanto l'ordinanza del Pretore di Lendin...   \n",
      "2                                               None   \n",
      "3  . 1. - Le tre ordinanze del Consiglio di Stato...   \n",
      "4  . 1. - Con l'ordinanza in epigrafe è sollevata...   \n",
      "\n",
      "                                           decisione  \n",
      "0  La Corte costituzionale riuniti i procedimenti...  \n",
      "1  La Corte costituzionale dichiara non fondate, ...  \n",
      "2  La Corte costituzionale riuniti i giudizi dich...  \n",
      "3  La Corte costituzionale dichiara non fondata l...  \n",
      "4  La Corte costituzionale Dichiara non fondata l...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "isLinux = True\n",
    "default_linux_path = os.path.join(os.getcwd().replace(\"/Data\", \"/Documents/Downloaded\")) if \"/Data\" in os.getcwd() else os.path.join(os.getcwd(), \"Documents\", \"Downloaded\")\n",
    "default_windows_path = os.path.join(os.getcwd().replace(\"\\\\Data\", \"\\\\Documents\\\\Downloaded\")) if \"\\\\Data\" in os.getcwd() else os.path.join(os.getcwd(), \"Documents\", \"Downloaded\")\n",
    "default_path = default_linux_path if isLinux else default_windows_path\n",
    "\n",
    "DEFAULT_SAVE_DIR = default_path.replace(\"/Downloaded\", \"/Generated\") if isLinux else default_path.replace(\"\\\\Downloaded\", \"\\\\Generated\")\n",
    "SENTENZE_JSONL = os.path.join(default_linux_path, 'corte_giacomo_rulings.jsonl')\n",
    "OUTPUT_JSONL = os.path.join(DEFAULT_SAVE_DIR, 'output_corte_rulings.jsonl')\n",
    "MISSING_JSONL = os.path.join(DEFAULT_SAVE_DIR, 'missing_corte_rulings.jsonl')\n",
    "\n",
    "output_dataset = []\n",
    "with open(SENTENZE_JSONL, 'r') as file:\n",
    "    for line in file:\n",
    "        output_dataset.append(json.loads(line.strip()))\n",
    "\n",
    "df = pd.DataFrame(output_dataset)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract laws references from text\n",
    "def extract_law_references(text):\n",
    "    if not text:\n",
    "        return []\n",
    "        \n",
    "    patterns = [\n",
    "    # Basic Law Reference (Article + Law Number + Year) DONT TOUCH\n",
    "    r'art\\. (\\d+).{0,15}legge (.{5,15}).{0,8}n\\. (\\d+)',\n",
    "    \n",
    "    # Article + Multiple Commas (Multiple Clauses)\n",
    "    r'articolo\\s*(\\d+),\\s*commi\\s*(?:\\w+)\\s*(?:e\\s*\\w+),?\\s*(?:del|della)?\\s*(d\\.?l\\.?|legge|d\\.?P\\.?R\\.?)\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})\\s*n\\.\\s*(\\d+)',\n",
    "    \n",
    "    # Short Form Law References (Only Article + Law)\n",
    "    r'art\\.?\\s*(\\d+)\\s*(?:u\\.c\\.)?\\s*legge\\s*n\\.\\s*(\\d+)',\n",
    "    \n",
    "    # D.P.R. Reference\n",
    "    r'd\\.?P\\.?R\\.?\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})\\s*n\\.\\s*(\\d+)',\n",
    "    \n",
    "    # Constitutional Law Reference\n",
    "    r'art\\.?\\s*(\\d+)\\s*commi\\s*(\\d+)\\s*e\\s*(\\d+)\\s*Cost\\.?',\n",
    "    \n",
    "    # Decree Law (D.L.) with Conversion to Law\n",
    "    r'd\\.?l\\.?\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})\\s*n\\.\\s*(\\d+)\\s*,\\s*conv\\.\\s*in\\s*legge\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})\\s*n\\.\\s*(\\d+)',\n",
    "    \n",
    "    # References to Multiple Articles\n",
    "    r'artt?\\.?\\s*(\\d+(?:,\\s*\\d+)*\\s*(?:e\\s*\\d+)?)\\s*legge\\s+(\\d{1,2}\\s+\\w+\\s+\\d{4})\\s*n\\.\\s*(\\d+)'\n",
    "    \n",
    "    # Others\n",
    "    r''\n",
    "    ]\n",
    "\n",
    "    references = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.finditer(pattern, text)\n",
    "        for match in matches:\n",
    "            # Extracting the various parts of the match\n",
    "            if pattern == patterns[0]:\n",
    "                # Basic Law Reference\n",
    "                art = match.group(1)\n",
    "                anno = match.group(2)\n",
    "                num = match.group(3)\n",
    "                organo = 'stato'\n",
    "                references.append({'art': art, 'comma': None, 'num': num, 'anno': anno, 'organo': organo})\n",
    "            \n",
    "            elif pattern == patterns[1]:\n",
    "                # Article + Multiple Commas\n",
    "                art = match.group(1)\n",
    "                comma = 'Multiple'\n",
    "                organo = match.group(2)\n",
    "                anno = match.group(3)\n",
    "                num = match.group(4)\n",
    "                references.append({'art': art, 'comma': comma, 'num': num, 'anno': anno, 'organo': organo})\n",
    "            \n",
    "            elif pattern == patterns[2]:\n",
    "                # Short Form Law Reference\n",
    "                art = match.group(1)\n",
    "                num = match.group(2)\n",
    "                organo = 'stato'\n",
    "                references.append({'art': art, 'comma': None, 'num': num, 'anno': None, 'organo': organo})\n",
    "            \n",
    "            elif pattern == patterns[3]:\n",
    "                # D.P.R. Reference\n",
    "                anno = match.group(1)\n",
    "                num = match.group(2)\n",
    "                organo = 'd.P.R.'\n",
    "                references.append({'art': None, 'comma': None, 'num': num, 'anno': anno, 'organo': organo})\n",
    "            \n",
    "            elif pattern == patterns[4]:\n",
    "                # Constitutional Law Reference\n",
    "                art = match.group(1)\n",
    "                comma = f'{match.group(2)} e {match.group(3)}'\n",
    "                organo = 'Cost'\n",
    "                references.append({'art': art, 'comma': comma, 'num': None, 'anno': None, 'organo': organo})\n",
    "            \n",
    "            elif pattern == patterns[5]:\n",
    "                # Decree Law with Conversion\n",
    "                art = None\n",
    "                anno = match.group(1)\n",
    "                num = match.group(2)\n",
    "                organo = 'd.l.'\n",
    "                references.append({'art': art, 'comma': None, 'num': num, 'anno': anno, 'organo': organo})\n",
    "                # Conversion part\n",
    "                conv_anno = match.group(3)\n",
    "                conv_num = match.group(4)\n",
    "                references.append({'art': art, 'comma': None, 'num': conv_num, 'anno': conv_anno, 'organo': 'legge'})\n",
    "            \n",
    "            elif pattern == patterns[6]:\n",
    "                # Multiple Articles\n",
    "                art = match.group(1)\n",
    "                anno = match.group(2)\n",
    "                num = match.group(3)\n",
    "                organo = 'stato'\n",
    "                references.append({'art': art, 'comma': None, 'num': num, 'anno': anno, 'organo': organo})\n",
    "    \n",
    "    return references\n",
    "\n",
    "# Function to process each JSONL record\n",
    "def process_record(record, missing_fields_list):\n",
    "    # Extract fields and parse law references\n",
    "    epigrafe_refs = extract_law_references(record.get(\"epigrafe\", \"\"))\n",
    "    ritenuto_in_fatto_refs = extract_law_references(record.get(\"ritenuto_in_fatto\", \"\"))\n",
    "    considerato_in_diritto_refs = extract_law_references(record.get(\"considerato_in_diritto\", \"\"))\n",
    "    decisione_refs = extract_law_references(record.get(\"decisione\", \"\"))\n",
    "    \n",
    "    # Check if any field is missing references\n",
    "    if not epigrafe_refs:\n",
    "        missing_fields_list.append((record.get(\"judg_id\"), \"epigrafe\"))\n",
    "    if not ritenuto_in_fatto_refs:\n",
    "        missing_fields_list.append((record.get(\"judg_id\"), \"ritenuto_in_fatto\"))\n",
    "    if not considerato_in_diritto_refs:\n",
    "        missing_fields_list.append((record.get(\"judg_id\"), \"considerato_in_diritto\"))\n",
    "    if not decisione_refs:\n",
    "        missing_fields_list.append((record.get(\"judg_id\"), \"decisione\"))\n",
    "    \n",
    "    # Return the processed record\n",
    "    processed_record = {\n",
    "        \"judg_id\": record.get(\"judg_id\"),\n",
    "        \"epigrafe\": epigrafe_refs,\n",
    "        \"ritenuto_in_fatto\": ritenuto_in_fatto_refs,\n",
    "        \"considerato_in_diritto\": considerato_in_diritto_refs,\n",
    "        \"decisione\": decisione_refs\n",
    "    }\n",
    "    \n",
    "    return processed_record\n",
    "\n",
    "# Main function to read JSONL, process, and write output\n",
    "def process_jsonl_file(input_file, output_file, missing_file):\n",
    "    missing_fields_list = []\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line.strip())\n",
    "            processed_record = process_record(record, missing_fields_list)\n",
    "            json.dump(processed_record, outfile)\n",
    "            outfile.write(\"\\n\")  # Write each processed record as a line\n",
    "\n",
    "    # Save the missing fields list to a file\n",
    "    with open(missing_file, 'w', encoding='utf-8') as missing_outfile:\n",
    "        for judg_id in missing_fields_list:\n",
    "            missing_outfile.write(f\"{judg_id}\\n\")\n",
    "    \n",
    "    # Optionally, return the missing fields list if needed\n",
    "    return missing_fields_list\n",
    "\n",
    "# Usage\n",
    "input_file = SENTENZE_JSONL\n",
    "output_file = OUTPUT_JSONL\n",
    "missing_file = MISSING_JSONL\n",
    "\n",
    "# Run the process\n",
    "missing_fields = process_jsonl_file(input_file, output_file, missing_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
