{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f38b9edb4243e5ba08e0d4b7de6484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from difflib import SequenceMatcher\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import pymilvus\n",
    "import numpy as np\n",
    "\n",
    "isLinux = True\n",
    "default_linux_path = os.getcwd().replace(\"/Data\", \"/Documents/Downloaded\") if \"/Data\" in os.getcwd() else os.getcwd() + \"/Documents/Downloaded\"\n",
    "default_windows_path = os.getcwd().replace(\"\\\\Data\", \"\\\\Documents\\\\Downloaded\") if \"\\\\Data\" in os.getcwd() else os.getcwd() + \"\\\\Documents\\\\Downloaded\"\n",
    "default_path = default_linux_path if isLinux else default_windows_path\n",
    "\n",
    "DEFAULT_SAVE_DIR = default_path.replace(\"/Downloaded\", \"/Generated\") if isLinux else default_path.replace(\"\\\\Downloaded\", \"\\\\Generated\")\n",
    "LAWS_CSV = DEFAULT_SAVE_DIR + ('/laws.csv' if isLinux else '\\\\laws.csv')\n",
    "\n",
    "# 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\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the best LLMs for answer legislative quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: Meta-Llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c5543a67444cf69e66ff35fed081c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/59.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1d347df37a4b5ead8778380fb8573a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a533213f5efc4e26b1a7d0c272c4e15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00030.safetensors:   0%|          | 0.00/4.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dffec7c0484b978e1be368b987ea74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25ea77b958c463c8abdc74b7633ab72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec56c430cc74590ade9990751a8651c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af75f05ca3642aead3ebc5ae79da00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fd99123fd941ebb40bcd953c1c52b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba6ef80de3048939a3d8134b4083b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdf8accfd3b4ca2946169f9f9c057f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a6051a6f7e4981a5ab3e110cc01fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26430deb892840999a95fcb85721cb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470464c1bea54102bd36c6039cb66215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b16c00a23214ecfb297e0f624003bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8fef395a7d46d69b66558db86c687f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f0c8a588a04095b34bad775501350e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479957994dae4dd78d13ab894770821b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 4664.17 MB. The target location /home/antonelli/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/blobs only has 1044.61 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cce16b4ba949d7bfafd293dc4f6e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ade3037677b4f30a80a3c43efd4b19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 4664.17 MB. The target location /home/antonelli/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-70B-Instruct/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21081bc445dc432d9376b1ed333f6c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00030.safetensors:  22%|##2       | 1.04G/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model meta-llama/Meta-Llama-3.1-70B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3693, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 1079, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3693, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 1079, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m model_id, context_window, prompt_function \u001b[38;5;241m=\u001b[39m model_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m], model_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_window\u001b[39m\u001b[38;5;124m'\u001b[39m], model_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_function\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m correct_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df_quiz\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:299\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    298\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m         )\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model meta-llama/Meta-Llama-3.1-70B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3693, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 1079, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/pipelines/base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 3693, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 1079, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/antonelli/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n"
     ]
    }
   ],
   "source": [
    "# List of models to test\n",
    "models = {\n",
    "    #\"Saul\": {'model_name': 'Equall/Saul-7B-Instruct-v1', 'context_window': 1024, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"}, #Modello addestrato su testi legali\n",
    "    #\"Llamantino\": {'model_name': 'swap-uniba/LLaMAntino-2-7b-hf-dolly-ITA', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"Di seguito Ã¨ riportata un'istruzione che descrive un'attivitÃ , abbinata ad un input che fornisce ulteriore informazione.\\nScrivi una risposta che soddisfi adeguatamente la richiesta.\\n\\n### Istruzione:\\n{system_prompt}\\n\\n### Input:\\n{user_prompt}\\n\\n### Risposta:\\n\"}, # Doesn't work with transformers\n",
    "    \"Meta-Llama\": {'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"},\n",
    "    \"Meta-Llama\": {'model_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"},\n",
    "    #\"Falcon-7B\": {'model_name': 'tiiuae/falcon-7b-instruc t', 'context_window': 512, 'prompt_function': lambda system_prompt, user_prompt: f\"User: {user_prompt}\\nAssistant:{system_prompt}\"},\n",
    "    #\"Mixtral-8x22B\": {'model_name': 'mistralai/Mixtral-8x22B-Instruct-v0.1', 'context_window': 1024, 'prompt_function': lambda system_prompt, user_prompt: f\"[INST] {system_prompt} {user_prompt}\\n[/INST]\"},\n",
    "    #\"Minerva-3B\": {'model_name': 'sapienzanlp/Minerva-3B-base-v1.0', 'context_window': 512, 'prompt_function': lambda system_prompt, user_prompt: f\"{system_prompt} {user_prompt}\"}, # Modello italiano della Sapienza\n",
    "    #\"deepset/roberta-base-squad2\" : {'model_name': 'deepset/roberta-base-squad2', 'context_window': 512}, # Modello per il question answering\n",
    "    #\"Phi-small\" : {'model_name': 'microsoft/Phi-3-small-4k-instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "    #\"Phi-medium\" : {'model_name': 'microsoft/Phi-3-medium-4k-instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "    #\"Phi-medium-quantized\" : {'model_name': 'kaitchup/Phi-3-medium-128k-instruct-awq-4bit', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "}\n",
    "\n",
    "df_quiz = pd.read_csv(DEFAULT_SAVE_DIR + '/quiz_merged.csv')\n",
    "\n",
    "# Initialize the models and generate answers\n",
    "for model_name, model_data in models.items():\n",
    "    model_id, context_window, prompt_function = model_data['model_name'], model_data['context_window'], model_data['prompt_function']\n",
    "    print(f'Running model: {model_name}')\n",
    "    \n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "\n",
    "    correct_count = 0\n",
    "    for index, row in df_quiz.iterrows():\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in the field of law, and you are gonna replay to the following quiz. You have to choose the correct answer among the three options. Just use the question and the answers as context.\"},\n",
    "            {\"role\": \"user\", \"content\": row['question']+row['answer_1']+row['answer_2']+row['answer_3']},\n",
    "        ]\n",
    "\n",
    "        # Generate answer\n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=1000,\n",
    "        )\n",
    "        ans = outputs[0][\"generated_text\"][-1]\n",
    "        \n",
    "        # Check which answer is more similar to the generated one\n",
    "        answers = [row['answer_1'], row['answer_2'], row['answer_3']]\n",
    "        similarities = [SequenceMatcher(None, ans, a).ratio() for a in answers]\n",
    "        most_similar_answer = answers[similarities.index(max(similarities))]\n",
    "        \n",
    "        # Check if the correct answer is within the generated answer\n",
    "        if most_similar_answer == row['answer_1']:\n",
    "            correct_count += 1\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = correct_count / len(df_quiz)\n",
    "    print(f'Accuracy of {model_name}: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test quizzes on Milvus RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_milvus():\n",
    "    connections.connect(host='localhost', port='19530')\n",
    "\n",
    "def create_collection():\n",
    "    laws_fields = [\n",
    "        FieldSchema(name=\"law_id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=128),\n",
    "        FieldSchema(name=\"source\", dtype=DataType.STRING, max_len=50),\n",
    "        FieldSchema(name=\"article\", dtype=DataType.INT64),\n",
    "        FieldSchema(name=\"comma\", dtype=DataType.INT64),\n",
    "        FieldSchema(name=\"comma_content\", dtype=DataType.STRING, max_len=5000)\n",
    "    ]    \n",
    "\n",
    "    laws_collection = Collection(name=\"laws_collection\", fields=laws_fields)\n",
    "    return laws_collection\n",
    "\n",
    "def load_data_and_generate_embeddings(LAWS_CSV):\n",
    "    data = pd.read_csv(LAWS_CSV)\n",
    "    model_name = \"BAAI/bge-m3\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for doc in data:\n",
    "        # Encode the text to get input ids & attention mask\n",
    "        encoded_input = tokenizer(doc, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "        # Get the embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        embeddings.append(model_output.last_hidden_state.mean(dim=1).numpy()[0])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def insert_data_into_milvus(collection, data, embeddings):\n",
    "    ids = [i for i in range(len(data))]\n",
    "    entities = [{\"name\": \"id\", \"values\": ids}, {\"name\": \"embedding\", \"values\": embeddings.tolist()}]\n",
    "    collection.insert(entities)\n",
    "\n",
    "def load_model():\n",
    "    llama3_model = 'llama3' # replace with actual llama3 model\n",
    "    llama3 = torch.load(llama3_model)\n",
    "    return llama3\n",
    "\n",
    "def initialize_RAG():\n",
    "    tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "    retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "    model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "    return tokenizer, model\n",
    "\n",
    "def prompt_and_retrieval(llama3, tokenizer, model, prompt):\n",
    "    encoded_prompt = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor(encoded_prompt).unsqueeze(0)\n",
    "    generated = model.generate(input_ids=input_ids, decoder_start_token_id=model.config.pad_token_id)\n",
    "    return generated\n",
    "\n",
    "connect_to_milvus()\n",
    "laws_collection = create_collection()\n",
    "data, embeddings = load_data_and_generate_embeddings('/path/to/your/csv')\n",
    "insert_data_into_milvus(collection=laws_collection, data=data, embeddings=embeddings)\n",
    "llama3 = load_model()\n",
    "tokenizer, model = initialize_RAG()\n",
    "prompt = \"your prompt here\"\n",
    "generated = prompt_and_retrieval(llama3=llama3, tokenizer=tokenizer, model=model, prompt=prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
