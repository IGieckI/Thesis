{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21d96463281474cb43648d468a8e88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from difflib import SequenceMatcher\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from pymilvus import connections, Collection, DataType, FieldSchema, CollectionSchema, utility\n",
    "from milvus import default_server\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "isLinux = True\n",
    "default_linux_path = os.getcwd().replace(\"/Data\", \"/Documents/Downloaded\") if \"/Data\" in os.getcwd() else os.getcwd() + \"/Documents/Downloaded\"\n",
    "default_windows_path = os.getcwd().replace(\"\\\\Data\", \"\\\\Documents\\\\Downloaded\") if \"\\\\Data\" in os.getcwd() else os.getcwd() + \"\\\\Documents\\\\Downloaded\"\n",
    "default_path = default_linux_path if isLinux else default_windows_path\n",
    "\n",
    "DEFAULT_SAVE_DIR = default_path.replace(\"/Downloaded\", \"/Generated\") if isLinux else default_path.replace(\"\\\\Downloaded\", \"\\\\Generated\")\n",
    "LAWS_CSV = DEFAULT_SAVE_DIR + ('/laws.csv' if isLinux else '\\\\laws.csv')\n",
    "\n",
    "# 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\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the best LLMs for answer legislative quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to test\n",
    "models = {\n",
    "    \"Saul\": {'model_name': 'Equall/Saul-7B-Instruct-v1', 'context_window': 1024, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"}, #Modello addestrato su testi legali\n",
    "    #\"Llamantino\": {'model_name': 'swap-uniba/LLaMAntino-2-7b-hf-dolly-ITA', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"Di seguito è riportata un'istruzione che descrive un'attività, abbinata ad un input che fornisce ulteriore informazione.\\nScrivi una risposta che soddisfi adeguatamente la richiesta.\\n\\n### Istruzione:\\n{system_prompt}\\n\\n### Input:\\n{user_prompt}\\n\\n### Risposta:\\n\"}, # Doesn't work with transformers\n",
    "    \"Meta-Llama 8B\": {'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"},\n",
    "    #\"Meta-Llama 70B\": {'model_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"},\n",
    "    \"Falcon-7B\": {'model_name': 'tiiuae/falcon-7b-instruc t', 'context_window': 512, 'prompt_function': lambda system_prompt, user_prompt: f\"User: {user_prompt}\\nAssistant:{system_prompt}\"},\n",
    "    #\"Mixtral-8x22B\": {'model_name': 'mistralai/Mixtral-8x22B-Instruct-v0.1', 'context_window': 1024, 'prompt_function': lambda system_prompt, user_prompt: f\"[INST] {system_prompt} {user_prompt}\\n[/INST]\"},\n",
    "    #\"Minerva-3B\": {'model_name': 'sapienzanlp/Minerva-3B-base-v1.0', 'context_window': 512, 'prompt_function': lambda system_prompt, user_prompt: f\"{system_prompt} {user_prompt}\"}, # Modello italiano della Sapienza\n",
    "    #\"deepset/roberta-base-squad2\" : {'model_name': 'deepset/roberta-base-squad2', 'context_window': 512}, # Modello per il question answering\n",
    "    #\"Phi-small\" : {'model_name': 'microsoft/Phi-3-small-4k-instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "    #\"Phi-medium\" : {'model_name': 'microsoft/Phi-3-medium-4k-instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "    #\"Phi-medium-quantized\" : {'model_name': 'kaitchup/Phi-3-medium-128k-instruct-awq-4bit', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "}\n",
    "\n",
    "df_quiz = pd.read_csv(DEFAULT_SAVE_DIR + '/quiz_merged.csv')\n",
    "\n",
    "# Initialize the models and generate answers\n",
    "for model_name, model_data in models.items():\n",
    "    model_id, context_window, prompt_function = model_data['model_name'], model_data['context_window'], model_data['prompt_function']\n",
    "    print(f'Running model: {model_name}')\n",
    "    \n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"cuda\",\n",
    "    )\n",
    "\n",
    "    correct_count = 0\n",
    "    for index, row in df_quiz.iterrows():\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in the field of law, and you are gonna replay to the following quiz. You have to choose the correct answer among the three options. Just use the question and the answers as context.\"},\n",
    "            {\"role\": \"user\", \"content\": row['question']+row['answer_1']+row['answer_2']+row['answer_3']},\n",
    "        ]\n",
    "\n",
    "        # Generate answer\n",
    "        outputs = pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=1000,\n",
    "        )\n",
    "        ans = outputs[0][\"generated_text\"][-1]\n",
    "        \n",
    "        # Check which answer is more similar to the generated one\n",
    "        answers = [row['answer_1'], row['answer_2'], row['answer_3']]\n",
    "        similarities = [SequenceMatcher(None, ans, a).ratio() for a in answers]\n",
    "        most_similar_answer = answers[similarities.index(max(similarities))]\n",
    "        \n",
    "        # Check if the correct answer is within the generated answer\n",
    "        if most_similar_answer == row['answer_1']:\n",
    "            correct_count += 1\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = correct_count / len(df_quiz)\n",
    "    print(f'Accuracy of {model_name}: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test quizzes on Milvus RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a866f1c468db45c8b51f37f5c89dfc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 3 has a total capacity of 23.68 GiB of which 1.09 GiB is free. Process 2438087 has 22.32 GiB memory in use. Including non-PyTorch memory, this process has 254.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#(\"BAAI/bge-m3\")\u001b[39;00m\n\u001b[1;32m    123\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:3\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    127\u001b[0m dataWithEmbeddings \u001b[38;5;241m=\u001b[39m load_data_and_generate_embeddings(pd\u001b[38;5;241m.\u001b[39mread_csv(LAWS_CSV), model, tokenizer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:2883\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2879\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[0;32m-> 2883\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 3 has a total capacity of 23.68 GiB of which 1.09 GiB is free. Process 2438087 has 22.32 GiB memory in use. Including non-PyTorch memory, this process has 254.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def connect_to_milvus():\n",
    "    try:\n",
    "        connections.connect(\"default\", host=\"0.0.0.0\")\n",
    "    except:\n",
    "        default_server.start()\n",
    "        \n",
    "def drop_everything():\n",
    "    collections = utility.list_collections()\n",
    "\n",
    "    for collection in collections:\n",
    "        utility.drop_collection(collection)\n",
    "\n",
    "def create_collection():\n",
    "    laws_fields = [\n",
    "        FieldSchema(name=\"law_id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=4096),\n",
    "        FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"article\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"comma\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"comma_content\", dtype=DataType.VARCHAR, max_length=5000)\n",
    "    ] \n",
    "\n",
    "    schema = CollectionSchema(laws_fields, \"laws collection\")\n",
    "\n",
    "    laws_collection = Collection(name=\"laws_collection\", schema=schema)\n",
    "    laws_collection.create_index(\"embedding\", {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}})    \n",
    "    \n",
    "    return laws_collection\n",
    "\n",
    "def load_model(model_name):\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    embedding_tensor = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    embedding_list = embedding_tensor.tolist()\n",
    "    \n",
    "    return embedding_list\n",
    "\n",
    "def load_data_and_generate_embeddings(data, model, tokenizer):\n",
    "    data = data[:3]\n",
    "\n",
    "    embeddings = []\n",
    "    for cc in tqdm(data[\"Comma content\"], total=data.shape[0]):\n",
    "        embeddings.append(generate_embedding(cc, tokenizer, model))\n",
    "    data[\"Embedding\"] = embeddings\n",
    "\n",
    "    return data\n",
    "\n",
    "def insert_data_into_milvus(collection, dataWithEmbeddings):\n",
    "    source_list = dataWithEmbeddings[\"Source\"].tolist()\n",
    "    article_list = dataWithEmbeddings[\"Article\"].tolist()\n",
    "    comma_list = dataWithEmbeddings[\"Comma number\"].tolist()\n",
    "    comma_content_list = dataWithEmbeddings[\"Comma content\"].tolist()\n",
    "    embedding_list = dataWithEmbeddings[\"Embedding\"].tolist()\n",
    "        \n",
    "    data = []\n",
    "    for i in range(len(embedding_list)):\n",
    "        data.append({\n",
    "            \"embedding\": embedding_list[i],     # Embedding (FLOAT_VECTOR)\n",
    "            \"source\": source_list[i],           # Source (VARCHAR)\n",
    "            \"article\": article_list[i],         # Article (VARCHAR)\n",
    "            \"comma\": comma_list[i],             # Comma (VARCHAR)\n",
    "            \"comma_content\": comma_content_list[i]  # Comma content (VARCHAR)\n",
    "        })\n",
    "    \n",
    "    collection.insert(data)\n",
    "    \n",
    "    collection.flush()\n",
    "    collection.load()\n",
    "    \n",
    "def search_similar_text(collection, text, tokenizer, model, top_k=5):    \n",
    "    # Generate the embedding for the input text\n",
    "    embedding = generate_embedding(text, tokenizer, model)\n",
    "    \n",
    "    # Perform a search on the collection\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    results = collection.search([embedding], \"embedding\", search_params, limit=top_k, output_fields=[\"source\", \"article\", \"comma\", \"comma_content\"])\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for result in results[0]:\n",
    "        formatted_results.append({\n",
    "            \"score\": result.score,\n",
    "            \"source\": result.entity.get(\"source\"),\n",
    "            \"article\": result.entity.get(\"article\"),\n",
    "            \"comma\": result.entity.get(\"comma\"),\n",
    "            \"comma_content\": result.entity.get(\"comma_content\")\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, device):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "connect_to_milvus()\n",
    "drop_everything() # !!! WARNING !!!\n",
    "laws_collection = create_collection()\n",
    "\n",
    "model, tokenizer = load_model(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")#(\"BAAI/bge-m3\")\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "dataWithEmbeddings = load_data_and_generate_embeddings(pd.read_csv(LAWS_CSV), model, tokenizer)\n",
    "insert_data_into_milvus(laws_collection, dataWithEmbeddings)\n",
    "\n",
    "while True:\n",
    "    user_prompt = \"Citami un articolo\"#input(\"Insert a prompt: \")\n",
    "    \n",
    "    search_results = search_similar_text(laws_collection, user_prompt, tokenizer, model)\n",
    "    print(search_results)\n",
    "\n",
    "    # Combine retrieved documents into a single context\n",
    "    context = \";\".join([result[\"comma_content\"] for result in search_results])\n",
    "    system_prompt = \"You are an expert in the field of law, and you are gonna replay to the following quiz. You have to choose the correct answer among the three options. These are some articles that could help you: \" + context\n",
    "\n",
    "    # Generate a final response using LLaMA 3 (optional, based on your needs)\n",
    "    response_input = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    response = generate_response(response_input, model, tokenizer, device)\n",
    "\n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
