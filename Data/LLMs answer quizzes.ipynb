{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee05df69e4d4ce892434b0ec87b4a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline, AutoModelForCausalLM\n",
    "from awq import AutoAWQForCausalLM\n",
    "import torch\n",
    "from difflib import SequenceMatcher\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from pymilvus import connections, Collection, DataType, FieldSchema, CollectionSchema, utility\n",
    "from milvus import default_server\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "isLinux = True\n",
    "default_linux_path = os.getcwd().replace(\"/Data\", \"/Documents/Downloaded\") if \"/Data\" in os.getcwd() else os.getcwd() + \"/Documents/Downloaded\"\n",
    "default_windows_path = os.getcwd().replace(\"\\\\Data\", \"\\\\Documents\\\\Downloaded\") if \"\\\\Data\" in os.getcwd() else os.getcwd() + \"\\\\Documents\\\\Downloaded\"\n",
    "default_path = default_linux_path if isLinux else default_windows_path\n",
    "\n",
    "DEFAULT_SAVE_DIR = default_path.replace(\"/Downloaded\", \"/Generated\") if isLinux else default_path.replace(\"\\\\Downloaded\", \"\\\\Generated\")\n",
    "LAWS_CSV = DEFAULT_SAVE_DIR + ('/laws.csv' if isLinux else '\\\\laws.csv')\n",
    "\n",
    "# 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\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the best LLMs for answer legislative quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to test\n",
    "models = {\n",
    "    \"Saul\": {'model_name': 'Equall/Saul-7B-Instruct-v1', 'context_window': 1024, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"}, #Modello addestrato su testi legali\n",
    "    #\"Llamantino\": {'model_name': 'swap-uniba/LLaMAntino-2-7b-hf-dolly-ITA', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"Di seguito è riportata un'istruzione che descrive un'attività, abbinata ad un input che fornisce ulteriore informazione.\\nScrivi una risposta che soddisfi adeguatamente la richiesta.\\n\\n### Istruzione:\\n{system_prompt}\\n\\n### Input:\\n{user_prompt}\\n\\n### Risposta:\\n\"}, # Doesn't work with transformers\n",
    "    \"Meta-Llama 8B\": {'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"},\n",
    "    #\"Meta-Llama 70B\": {'model_name': 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"},\n",
    "    \"Falcon-7B\": {'model_name': 'tiiuae/falcon-7b-instruc t', 'context_window': 512, 'prompt_function': lambda system_prompt, user_prompt: f\"User: {user_prompt}\\nAssistant:{system_prompt}\"},\n",
    "    #\"Mixtral-8x22B\": {'model_name': 'mistralai/Mixtral-8x22B-Instruct-v0.1', 'context_window': 1024, 'prompt_function': lambda system_prompt, user_prompt: f\"[INST] {system_prompt} {user_prompt}\\n[/INST]\"},\n",
    "    #\"Minerva-3B\": {'model_name': 'sapienzanlp/Minerva-3B-base-v1.0', 'context_window': 512, 'prompt_function': lambda system_prompt, user_prompt: f\"{system_prompt} {user_prompt}\"}, # Modello italiano della Sapienza\n",
    "    #\"deepset/roberta-base-squad2\" : {'model_name': 'deepset/roberta-base-squad2', 'context_window': 512}, # Modello per il question answering\n",
    "    #\"Phi-small\" : {'model_name': 'microsoft/Phi-3-small-4k-instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "    #\"Phi-medium\" : {'model_name': 'microsoft/Phi-3-medium-4k-instruct', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "    #\"Phi-medium-quantized\" : {'model_name': 'kaitchup/Phi-3-medium-128k-instruct-awq-4bit', 'context_window': 8000, 'prompt_function': lambda system_prompt, user_prompt: f\"<|system|>\\n{system_prompt}\\n|<user>|\\n{user_prompt}\\n|<assistant>|\\n\\n\"},\n",
    "}\n",
    "\n",
    "df_quiz = pd.read_csv(DEFAULT_SAVE_DIR + '/quiz_merged.csv')\n",
    "df_ref = pd.read_csv(DEFAULT_SAVE_DIR + '/references_merged.csv')\n",
    "\n",
    "# Initialize the models and generate answers\n",
    "for model_name, model_data in models.items():\n",
    "    model_id, context_window, prompt_function = model_data['model_name'], model_data['context_window'], model_data['prompt_function']\n",
    "    print(f'Running model: {model_name}')\n",
    "    \n",
    "    # Load the model using AutoAWQForCausalLM\n",
    "    quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        **{\"low_cpu_mem_usage\": True}, \n",
    "        device_map=\"cuda\",  \n",
    "        trust_remote_code = True,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    model.quantize(tokenizer, quant_config=quant_config)   \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    correct_count = 0\n",
    "    for index, row in df_quiz.iterrows():\n",
    "        # Check for references\n",
    "        ref = df_ref[df_ref['Question id'] == row['index']]\n",
    "        if ref.empty:\n",
    "            continue\n",
    "        \n",
    "        # Use the prompt function to format the input\n",
    "        input_text = prompt_function(\n",
    "            \"You are an expert in the field of law, and you are gonna reply to the following quiz. You have to choose the correct answer among the three options. Just use the question and the answers as context. This is the referenced article in the question: \" \n",
    "            + f\"n. {ref.iloc[0]['Reference']} from {ref.iloc[0]['Source']}{ \" comma\"+ref.iloc[0]['Comma'] if ref.iloc[0]['Comma'] != \"\" else \"\" }\",\n",
    "            row['question'] + row['answer_1'] + row['answer_2'] + row['answer_3']\n",
    "        )\n",
    "\n",
    "        # Generate answer\n",
    "        outputs = pipeline(\n",
    "            input_text,\n",
    "            max_new_tokens=1000,\n",
    "        )\n",
    "        ans = outputs[0][\"generated_text\"]\n",
    "\n",
    "        # Check which answer is more similar to the generated one\n",
    "        answers = [row['answer_1'], row['answer_2'], row['answer_3']]\n",
    "        similarities = [SequenceMatcher(None, ans, a).ratio() for a in answers]\n",
    "        most_similar_answer = answers[similarities.index(max(similarities))]\n",
    "        \n",
    "        # Check if the correct answer is within the generated answer\n",
    "        if most_similar_answer == row['answer_1']:\n",
    "            correct_count += 1\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = correct_count / len(df_quiz)\n",
    "    print(f'Accuracy of {model_name}: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test quizzes on Milvus RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "MilvusException",
     "evalue": "<MilvusException: (code=2, message=Fail connecting to server on 0.0.0.0:19531, illegal connection params or server unavailable)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFutureTimeoutError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pymilvus/client/grpc_handler.py:147\u001b[0m, in \u001b[0;36mGrpcHandler._wait_for_channel_ready\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[43mgrpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_ready_future\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_channel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_identifier_interceptor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/grpc/_utilities.py:162\u001b[0m, in \u001b[0;36m_ChannelReadyFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/grpc/_utilities.py:106\u001b[0m, in \u001b[0;36m_ChannelReadyFuture._block\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mFutureTimeoutError()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFutureTimeoutError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMilvusException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m     response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m--> 129\u001b[0m \u001b[43mconnect_to_milvus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m drop_everything() \u001b[38;5;66;03m# !!! WARNING !!!\u001b[39;00m\n\u001b[1;32m    131\u001b[0m laws_collection \u001b[38;5;241m=\u001b[39m create_collection()\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mconnect_to_milvus\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect_to_milvus\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mconnections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m19531\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pymilvus/orm/connections.py:449\u001b[0m, in \u001b[0;36mConnections.connect\u001b[0;34m(self, alias, user, password, db_name, token, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed_uri\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    447\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecure\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[43mconnect_milvus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# 2nd Priority, connection configs from env\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pymilvus/orm/connections.py:400\u001b[0m, in \u001b[0;36mConnections.connect.<locals>.connect_milvus\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m t \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    398\u001b[0m timeout \u001b[38;5;241m=\u001b[39m t \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m Config\u001b[38;5;241m.\u001b[39mMILVUS_CONN_TIMEOUT\n\u001b[0;32m--> 400\u001b[0m \u001b[43mgh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_channel_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    402\u001b[0m     gh\u001b[38;5;241m.\u001b[39mregister_state_change_callback(\n\u001b[1;32m    403\u001b[0m         ReconnectHandler(\u001b[38;5;28mself\u001b[39m, alias, kwargs_copy)\u001b[38;5;241m.\u001b[39mreconnect_on_idle\n\u001b[1;32m    404\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pymilvus/client/grpc_handler.py:150\u001b[0m, in \u001b[0;36mGrpcHandler._wait_for_channel_ready\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_identifier_interceptor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mFutureTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MilvusException(\n\u001b[1;32m    151\u001b[0m         code\u001b[38;5;241m=\u001b[39mStatus\u001b[38;5;241m.\u001b[39mCONNECT_FAILED,\n\u001b[1;32m    152\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFail connecting to server on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_address\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, illegal connection params or server unavailable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mMilvusException\u001b[0m: <MilvusException: (code=2, message=Fail connecting to server on 0.0.0.0:19531, illegal connection params or server unavailable)>"
     ]
    }
   ],
   "source": [
    "def connect_to_milvus():\n",
    "    print(1)\n",
    "    try:\n",
    "        print(2)\n",
    "        connections.connect(\"default\", host=\"0.0.0.0\", port=\"19531\")\n",
    "        print(2)\n",
    "    except:\n",
    "        print(3)\n",
    "        default_server.start()\n",
    "        print(3)\n",
    "    print(4)\n",
    "\n",
    "def drop_everything():\n",
    "    collections = utility.list_collections()\n",
    "\n",
    "    for collection in collections:\n",
    "        utility.drop_collection(collection)\n",
    "\n",
    "def create_collection():\n",
    "    laws_fields = [\n",
    "        FieldSchema(name=\"law_id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=4096),\n",
    "        FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"article\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"comma\", dtype=DataType.VARCHAR, max_length=50),\n",
    "        FieldSchema(name=\"comma_content\", dtype=DataType.VARCHAR, max_length=5000)\n",
    "    ] \n",
    "\n",
    "    schema = CollectionSchema(laws_fields, \"laws collection\")\n",
    "\n",
    "    laws_collection = Collection(name=\"laws_collection\", schema=schema)\n",
    "    laws_collection.create_index(\"embedding\", {\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\", \"params\": {\"nlist\": 128}})    \n",
    "    \n",
    "    return laws_collection\n",
    "\n",
    "def load_model(model_name):    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #model = AutoModel.from_pretrained(model_name)\n",
    "    #quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "    #model = AutoAWQForCausalLM.from_pretrained(model_name, **{\"low_cpu_mem_usage\": True}, device_map=\"cuda:1\",  trust_remote_code = True)\n",
    "    #model.quantize(tokenizer, quant_config=quant_config)    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"cuda\")\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    embedding_tensor = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    embedding_list = embedding_tensor.tolist()\n",
    "    \n",
    "    return embedding_list\n",
    "\n",
    "def load_data_and_generate_embeddings(data, model, tokenizer):\n",
    "    data = data[:3]\n",
    "\n",
    "    embeddings = []\n",
    "    for cc in tqdm(data[\"Comma content\"], total=data.shape[0]):\n",
    "        embeddings.append(generate_embedding(cc, tokenizer, model))\n",
    "    data[\"Embedding\"] = embeddings\n",
    "\n",
    "    return data\n",
    "\n",
    "def insert_data_into_milvus(collection, dataWithEmbeddings):\n",
    "    source_list = dataWithEmbeddings[\"Source\"].tolist()\n",
    "    article_list = dataWithEmbeddings[\"Article\"].tolist()\n",
    "    comma_list = dataWithEmbeddings[\"Comma number\"].tolist()\n",
    "    comma_content_list = dataWithEmbeddings[\"Comma content\"].tolist()\n",
    "    embedding_list = dataWithEmbeddings[\"Embedding\"].tolist()\n",
    "        \n",
    "    data = []\n",
    "    for i in range(len(embedding_list)):\n",
    "        data.append({\n",
    "            \"embedding\": embedding_list[i],     # Embedding (FLOAT_VECTOR)\n",
    "            \"source\": source_list[i],           # Source (VARCHAR)\n",
    "            \"article\": article_list[i],         # Article (VARCHAR)\n",
    "            \"comma\": comma_list[i],             # Comma (VARCHAR)\n",
    "            \"comma_content\": comma_content_list[i]  # Comma content (VARCHAR)\n",
    "        })\n",
    "    \n",
    "    collection.insert(data)\n",
    "    \n",
    "    collection.flush()\n",
    "    collection.load()\n",
    "    \n",
    "def search_similar_text(collection, text, tokenizer, model, top_k=5):\n",
    "    # Generate the embedding for the input text\n",
    "    embedding = generate_embedding(text, tokenizer, model)\n",
    "    \n",
    "    # Perform a search on the collection\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    results = collection.search([embedding], \"embedding\", search_params, limit=top_k, output_fields=[\"source\", \"article\", \"comma\", \"comma_content\"])\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for result in results[0]:\n",
    "        formatted_results.append({\n",
    "            \"score\": result.score,\n",
    "            \"source\": result.entity.get(\"source\"),\n",
    "            \"article\": result.entity.get(\"article\"),\n",
    "            \"comma\": result.entity.get(\"comma\"),\n",
    "            \"comma_content\": result.entity.get(\"comma_content\")\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, device):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "connect_to_milvus()\n",
    "drop_everything() # !!! WARNING !!!\n",
    "laws_collection = create_collection()\n",
    "\n",
    "model, tokenizer = load_model(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")#(\"BAAI/bge-m3\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "#model.eval()\n",
    "\n",
    "dataWithEmbeddings = load_data_and_generate_embeddings(pd.read_csv(LAWS_CSV), model, tokenizer)\n",
    "insert_data_into_milvus(laws_collection, dataWithEmbeddings)\n",
    "\n",
    "while True:\n",
    "    user_prompt = \"Citami un articolo\"#input(\"Insert a prompt: \")\n",
    "    \n",
    "    search_results = search_similar_text(laws_collection, user_prompt, tokenizer, model)\n",
    "    print(search_results)\n",
    "\n",
    "    # Combine retrieved documents into a single context\n",
    "    context = \";\".join([result[\"comma_content\"] for result in search_results])\n",
    "    system_prompt = \"You are an expert in the field of law, and you are gonna replay to the following quiz. You have to choose the correct answer among the three options. These are some articles that could help you: \" + context\n",
    "\n",
    "    # Generate a final response using LLaMA 3 (optional, based on your needs)\n",
    "    response_input = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    response = generate_response(response_input, model, tokenizer, device)\n",
    "\n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
