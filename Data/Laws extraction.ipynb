{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lxml.etree as ET\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "isLinux = True\n",
    "default_linux_path = os.getcwd().replace(\"/Data\", \"/Documents/Downloaded\") if \"/Data\" in os.getcwd() else os.getcwd() + \"/Documents/Downloaded\"\n",
    "default_windows_path = os.getcwd().replace(\"\\\\Data\", \"\\\\Documents\\\\Downloaded\") if \"\\\\Data\" in os.getcwd() else os.getcwd() + \"\\\\Documents\\\\Downloaded\"\n",
    "default_path = default_linux_path if isLinux else default_windows_path\n",
    "\n",
    "DEFAULT_SAVE_DIR = default_path.replace(\"/Downloaded\", \"/Generated\") if isLinux else default_path.replace(\"\\\\Downloaded\", \"\\\\Generated\")\n",
    "CHROME_DRIVERS_PATH = \"/home/antonelli2/chromedriver-linux64/chromedriver\" if isLinux else \"C:\\\\Users\\\\giaco\\\\Downloads\\\\chromedriver-win64\\\\chromedriver.exe\"\n",
    "# Unibo: /chromedriver-linux64/chromedriver or /home/antonelli2/chromedriver-linux64/chromedriver\n",
    "# WSL: /home/giacomo/chromedriver-linux64/chromedriver\n",
    "\n",
    "COSTITUZIONE_CSV = DEFAULT_SAVE_DIR + (\"/Costituzione.csv\" if isLinux else \"\\\\Costituzione.csv\")\n",
    "\n",
    "CODICE_PENALE_PDF = default_path + (\"/Codice penale well formatted edited.pdf\" if isLinux else \"\\\\Codice penale well formatted edited.pdf\")\n",
    "CODICE_PENALE_CSV = DEFAULT_SAVE_DIR + (\"/Codice penale.csv\" if isLinux else \"\\\\Codice penale.csv\")\n",
    "\n",
    "CPP_CSV = DEFAULT_SAVE_DIR + (\"/Codice procedura penale.csv\" if isLinux else \"\\\\Codice procedura penale.csv\")\n",
    "\n",
    "CPA_CSV = DEFAULT_SAVE_DIR + (\"/Codice processo amministrativo.csv\" if isLinux else \"\\\\Codice procedura amministrativo.csv\")\n",
    "\n",
    "REF_MERG = DEFAULT_SAVE_DIR + ('/references_merged.csv' if isLinux else '\\\\references_merged.csv')\n",
    "ALREADY_SCRAPED_DLGS_JSON = DEFAULT_SAVE_DIR + ('/scraped_dlgs.json' if isLinux else '\\\\scraped_dlgs.json')\n",
    "DLGS_CSV = DEFAULT_SAVE_DIR + ('/dlgs.csv' if isLinux else '\\\\dlgs.csv')\n",
    "\n",
    "ALL_ITALIAN_LAWS_CSV = DEFAULT_SAVE_DIR + (\"/All Italian laws.csv\" if isLinux else \"\\\\All Italian laws.csv\")\n",
    "ALL_ITALIAN_LAWS_SCRAPED_JSON = DEFAULT_SAVE_DIR + (\"/All laws.json\" if isLinux else \"\\\\All laws.json\")\n",
    "\n",
    "LAWS_CSV = DEFAULT_SAVE_DIR + (\"/laws.csv\" if isLinux else \"\\\\laws.csv\")\n",
    "\n",
    "\n",
    "# Utility functions and constants\n",
    "def write_to_file(filename, content):\n",
    "    with open(filename, 'w+') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def read_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "# JSON stuff\n",
    "def write_list_to_json(lst, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(lst, json_file)\n",
    "\n",
    "def read_list_from_json(filename):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        py_list = json.load(json_file)\n",
    "    return py_list\n",
    "\n",
    "# CSV stuff\n",
    "def save_df_to_csv(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def read_df_from_csv(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "# Different kind of text extraction from each type of file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text    \n",
    "\n",
    "def clearLawContent(content):\n",
    "    # Check for comma class tags\n",
    "    matches =  re.findall(r'<span class=\"art_text_in_comma\">(.*?)</span>', content, re.DOTALL)\n",
    "    content = ' '.join(matches) if matches else content\n",
    "    content = re.sub(r\"<div class=\\\"ins-akn\\\" eid=\\\"ins_\\d+\\\">\\(\\(\", \"\", content, flags=re.DOTALL)\n",
    "    content = re.sub(r\"\\n\", \"\", content, flags=re.DOTALL)\n",
    "    content = re.sub(r\"<br>\", \"\", content, flags=re.DOTALL)\n",
    "\n",
    "    \n",
    "    # Check for a tags\n",
    "    aPattern = re.compile(r'<a.*?>(.*?)</a>', re.DOTALL)\n",
    "    matches = aPattern.findall(content)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            content = re.sub(r'<a.*?>.*?</a>', match, content, count=1)\n",
    "\n",
    "    # Check for span tags\n",
    "    sPattern = re.compile(r'<span.*?>(.*?)</span>', re.DOTALL)\n",
    "    matches = sPattern.findall(content)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            content = re.sub(r'<span.*?>.*?</span>', match, content, count=1)\n",
    "\n",
    "    # Check for list div tags\n",
    "    dlPattern = re.compile(r'<div class=\"pointedList-rest-akn\">(.*?)</div>', re.DOTALL)\n",
    "    matches = dlPattern.findall(content)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            content = re.sub(r'<div class=\"pointedList-rest-akn\">.*?</div>', re.escape(match), content, count=1)\n",
    "    \n",
    "    # Delete remaining tags\n",
    "    content = re.sub(r'<.+?>', \"\", content)\n",
    "    content = content.replace(\"\\n\", \"\")\n",
    "    content = content.replace(\"((\", \"\")\n",
    "    content = content.replace(\"))\", \"\")\n",
    "    \n",
    "    return content.strip()\n",
    "\n",
    "def extractCommaNumber(articleElement):\n",
    "    articleElement = articleElement.strip()\n",
    "    if \"art.\" in articleElement:\n",
    "        return articleElement.split(\" \")[1]\n",
    "    return articleElement\n",
    "\n",
    "def extractArticleNumber(articleElement):\n",
    "    match = re.search(r'n\\..*?(\\d+)', articleElement)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    raise Exception(\"Article number not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Constitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormattivaCosScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    # Get the text of a specific article\n",
    "    def get_cos_articles(self):\n",
    "        articles_list = []\n",
    "        \n",
    "        # Ensure is multivigente version\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        #print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for i, article in enumerate(articles):\n",
    "            # if there is any letter in article skip it\n",
    "            if any(char.isalpha() for char in article.text):\n",
    "                continue\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Somehow some crash, just add them by hand\n",
    "            article.click()\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                text = self.driver.find_element(By.CLASS_NAME, \"art-just-text-akn\")\n",
    "                law_content = text.get_attribute('outerHTML')\n",
    "                law_content = law_content.replace(\"\\n\", \"\")\n",
    "            except:\n",
    "                print(\"Error in article: \", article.text)\n",
    "                # continue\n",
    "                return articles_list\n",
    "            \n",
    "            law_number = article.text\n",
    "            law_content = clearLawContent(law_content)            \n",
    "\n",
    "            # Clear the output\n",
    "            law_content = clearLawContent(law_content)\n",
    "            articles_list.append({ \"law_source\": \"cos\",\n",
    "                                    \"year\": None,\n",
    "                                    \"law_number\": law_number,\n",
    "                                    \"law_text\": law_content })\n",
    "            print(articles_list[-1])\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "scraper = NormattivaCosScraper(CHROME_DRIVERS_PATH, headless=False)\n",
    "scraper.navigate_to_page(\"https://www.normattiva.it/uri-res/N2Ls?urn:nir:stato:costituzione\")\n",
    "articles = scraper.get_cos_articles()\n",
    "\n",
    "df_cos = pd.DataFrame(articles)\n",
    "df_cos.to_csv(COSTITUZIONE_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Codice Penale from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/common/driver_finder.py:64\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe path is not a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m path\n",
      "\u001b[0;31mValueError\u001b[0m: The path is not a valid file: /home/giacomo/chromedriver-linux64/chromedriver",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m---> 78\u001b[0m scraper \u001b[38;5;241m=\u001b[39m \u001b[43mNormattivaCpScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHROME_DRIVERS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m scraper\u001b[38;5;241m.\u001b[39mnavigate_to_page(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.normattiva.it/uri-res/N2Ls?urn:nir:stato:regio.decreto:1930-10-19;1398\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m articles \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mget_cp_articles()\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mNormattivaCpScraper.__init__\u001b[0;34m(self, driver_path, headless)\u001b[0m\n\u001b[1;32m      7\u001b[0m     chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--no-sandbox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m Service(driver_path)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/chromium/webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m     49\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     51\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[1;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/common/driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/common/driver_finder.py:78\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     77\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths\n",
      "\u001b[0;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "class NormattivaCpScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    # Get the text of a specific article\n",
    "    def get_cp_articles(self):\n",
    "        articles_list = []\n",
    "        \n",
    "        # Ensure is multivigente version\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        #print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for i, article in enumerate(articles[1:]):\n",
    "            \n",
    "            time.sleep(1)\n",
    "            if \"art\" in article.text or \"Art\" in article.text:\n",
    "                \n",
    "                article.click()\n",
    "                time.sleep(1)\n",
    "                try:\n",
    "                    text = self.driver.find_elements(By.CLASS_NAME, \"attachment-just-text\")[0]\n",
    "                    law_content = text.get_attribute('outerHTML')\n",
    "                    law_content = law_content.replace(\"\\n\", \"\")\n",
    "                except:\n",
    "                    print(\"Error in article: \", article.text)\n",
    "                    # continue\n",
    "                    return articles_list\n",
    "                \n",
    "                # Extract the article number\n",
    "                title_pattern = r'>.+(Art\\..+)<'\n",
    "                match = re.search(title_pattern, law_content, re.DOTALL)\n",
    "                                \n",
    "                # If a match is found, print it\n",
    "                if match:\n",
    "                    law_number = match.group(1).split(\"<\")[0]\n",
    "                    law_content = law_content.split(law_number)[1]\n",
    "                else:\n",
    "                    print(text)\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                # Clear the output\n",
    "                law_content = clearLawContent(law_content)\n",
    "                articles_list.append({ \"law_source\": \"c.p.\",\n",
    "                                       \"law_text\": law_content,\n",
    "                                       \"law_number\": law_number,\n",
    "                                       \"year\": None })\n",
    "                print(articles_list[-1])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "scraper = NormattivaCpScraper(CHROME_DRIVERS_PATH, headless=False)\n",
    "scraper.navigate_to_page(\"https://www.normattiva.it/uri-res/N2Ls?urn:nir:stato:regio.decreto:1930-10-19;1398\")\n",
    "articles = scraper.get_cp_articles()\n",
    "\n",
    "df_cp = pd.DataFrame(articles)\n",
    "df_cp.to_csv(CODICE_PENALE_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of C.P.P. from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormattivaCppScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    def get_cpp_articles(self):\n",
    "        articles_list = []\n",
    "        \n",
    "        # Ensure is multivigente version\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        #print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for i, article in enumerate(articles[1:]):\n",
    "            \n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                article.click()\n",
    "            except:\n",
    "                continue\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                law_number = self.driver.find_elements(By.CLASS_NAME, \"article-num-akn\")[0]\n",
    "                law_number = law_number.get_attribute('outerHTML')\n",
    "                law_number = law_number.replace(\"\\n\", \"\")\n",
    "                law_number = clearLawContent(law_number)\n",
    "                try:                    \n",
    "                    law_content = self.driver.find_elements(By.CLASS_NAME, \"art-commi-div-akn\")[0]\n",
    "                except:\n",
    "                    law_content = self.driver.find_elements(By.CLASS_NAME, \"art-just-text-akn\")[0]\n",
    "                law_content = law_content.get_attribute('outerHTML')                \n",
    "                law_content = law_content.replace(\"\\n\", \"\")\n",
    "                law_content = clearLawContent(law_content)\n",
    "            except:\n",
    "                print(\"Error in article: \", article.text)\n",
    "                # continue\n",
    "                return articles_list\n",
    "            \n",
    "            # Clear the output\n",
    "            articles_list.append({ \"law_source\": \"c.p.p.\",\n",
    "                                    \"law_text\": law_content,\n",
    "                                    \"law_number\": law_number,\n",
    "                                    \"year\": None })\n",
    "            print(articles_list[-1])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "scraper = NormattivaCppScraper(CHROME_DRIVERS_PATH, headless=False)\n",
    "scraper.navigate_to_page(\"https://www.normattiva.it/uri-res/N2Ls?urn:nir:stato:decreto.del.presidente.della.repubblica:1988-09-22;447\")\n",
    "articles = scraper.get_cpp_articles()\n",
    "\n",
    "df_cpp = pd.DataFrame(articles)\n",
    "df_cpp.to_csv(CPP_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of C.P.A. from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormattivaCpaScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    def get_cpa_articles(self):\n",
    "        articles_list = []\n",
    "        \n",
    "        # Ensure is multivigente version\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        #print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for i, article in enumerate(articles[1:]):\n",
    "            if \"art\" not in article.text:\n",
    "                continue\n",
    "            \n",
    "            print(\"Article: \", article.text)\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                article.click()\n",
    "            except:\n",
    "                continue\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                law_number = article.text[5:]\n",
    "                                \n",
    "                try:                    \n",
    "                    law_content = self.driver.find_element(By.CLASS_NAME, \"art-commi-div-akn\")\n",
    "                except:\n",
    "                    law_content = self.driver.find_element(By.CLASS_NAME, \"attachment-just-text\")\n",
    "                law_content = law_content.get_attribute('outerHTML')                \n",
    "                law_content = law_content.replace(\"\\n\", \"\")\n",
    "                law_content = clearLawContent(law_content)\n",
    "            except:\n",
    "                print(\"Error in article: \", article.text)\n",
    "                # continue\n",
    "                return articles_list\n",
    "            \n",
    "            # Clear the output\n",
    "            articles_list.append({ \"law_source\": \"c.p.a.\",\n",
    "                                    \"year\": None,\n",
    "                                    \"law_number\": law_number,\n",
    "                                    \"law_text\": law_content})\n",
    "            print(articles_list[-1])\n",
    "\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "scraper = NormattivaCpaScraper(CHROME_DRIVERS_PATH, headless=False)\n",
    "scraper.navigate_to_page(\"https://www.normattiva.it/uri-res/N2Ls?urn:nir:stato:decreto.legislativo:2010-07-02;104\")\n",
    "articles = scraper.get_cpa_articles()\n",
    "\n",
    "df_cpa = pd.DataFrame(articles)\n",
    "df_cpa.to_csv(CPA_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Dlgs from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormattivaDlgsScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    # Get the text of a specific article\n",
    "    def get_laws(self, numeroProvvedimento, anno):\n",
    "        articles_list = []\n",
    "            \n",
    "        self.fill_field(\"numeroProvvedimento\", numeroProvvedimento)\n",
    "        self.fill_field(\"annoProvvedimento\", anno)\n",
    "\n",
    "        self.driver.find_element(By.CSS_SELECTOR, \"[type*='submit']\").click()\n",
    "        self.driver.find_elements(By.CSS_SELECTOR, \"[title*='Dettaglio atto']\")[0].click()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        # Ensure is multivigente version\n",
    "        multivigente_button = self.driver.find_element(By.XPATH, '//a[contains(@href, \"multivigenza\")]')\n",
    "        multivigente_button.click()\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for article in articles:\n",
    "            if article.text.strip() != \"\" and article.text[0].isdigit() and \"orig\" not in article.text and \"Allegato\" not in article.text and \"agg\" not in article.text:\n",
    "                print(\"In \", article.text)\n",
    "                try:\n",
    "                    article.click()\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "                #article_title = self.driver.find_element(By.CLASS_NAME, \"article-num-akn\").text.strip()\n",
    "                #commas = self.driver.find_elements(By.CLASS_NAME, \"art-comma-div-akn\")\n",
    "                #page_title =  scraper.driver.title\n",
    "                #law_number = extractArticleNumber(page_title)\n",
    "                law_number = article.text\n",
    "                \n",
    "                try:\n",
    "                    law_text = self.driver.find_element(By.CLASS_NAME, \"art-commi-div-akn\").get_attribute('outerHTML')\n",
    "                except:\n",
    "                    # Se l'articolo è stato abrogato la pagina è diversa\n",
    "                    law_text = self.driver.find_element(By.CLASS_NAME, \"art-just-text-akn\").get_attribute('outerHTML')\n",
    "                law_text = clearLawContent(law_text)\n",
    "                \n",
    "                if law_text == \"\":\n",
    "                    continue\n",
    "                \n",
    "                articles_list.append({ \"law_source\": f\"Dlgs {numeroProvvedimento}/{anno}\".strip(),\n",
    "                                       \"year\": None,\n",
    "                                       \"law_number\": law_number,\n",
    "                                       \"law_text\": law_text})\n",
    "                \n",
    "            else:\n",
    "                print(\"Out \", article.text)\n",
    "\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "scraper = NormattivaDlgsScraper(CHROME_DRIVERS_PATH, headless=False)\n",
    "\n",
    "# Dataframe of already been scraped laws\n",
    "df_dlgs = pd.DataFrame()\n",
    "if os.path.exists(DLGS_CSV):\n",
    "    df_dlgs = pd.read_csv(DLGS_CSV)\n",
    "\n",
    "# List of already been scraped dlgs\n",
    "scraped_dlgs = []\n",
    "if os.path.exists(ALREADY_SCRAPED_DLGS_JSON):\n",
    "    scraped_dlgs = read_list_from_json(ALREADY_SCRAPED_DLGS_JSON)\n",
    "\n",
    "df_ref = pd.read_csv(REF_MERG)\n",
    "\n",
    "for index, row in tqdm(df_ref.iterrows(), total=df_ref.shape[0]): # 445\n",
    "    # Check if it's a D. lgs.\n",
    "    if '/' not in row['law_source'] or row['law_source'].strip() in scraped_dlgs:\n",
    "        continue\n",
    "    scraped_dlgs.append(row['law_source'])\n",
    "    \n",
    "    print(f\"Scraping |{row['law_source']}|\")\n",
    "    num, year = row['law_source'].split(\"/\")\n",
    "    \n",
    "    # some year references count \"2\" or \"3\" like \"2002\" or \"2003\"\n",
    "    year = year if len(year) == 4 else f\"200{year.strip()}\".strip()\n",
    "    \n",
    "    scraper.navigate_to_page(\"https://www.normattiva.it/ricerca/avanzata\")\n",
    "    \n",
    "    articles = scraper.get_laws(num, year)\n",
    "    \n",
    "    if not articles:\n",
    "        continue\n",
    "    \n",
    "    df_dlgs = pd.concat([df_dlgs, pd.DataFrame(articles)], ignore_index=True)\n",
    "     \n",
    "    save_df_to_csv(df_dlgs, DLGS_CSV)\n",
    "    write_list_to_json(scraped_dlgs, ALREADY_SCRAPED_DLGS_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of all italian's laws from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SessionNotCreatedException",
     "evalue": "Message: session not created: Chrome failed to start: exited normally.\n  (session not created: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /opt/google/chrome/chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x5638ee3aaf33 <unknown>\n#1 0x5638ee0a2ce6 <unknown>\n#2 0x5638ee0d76fa <unknown>\n#3 0x5638ee0d36ed <unknown>\n#4 0x5638ee11c71c <unknown>\n#5 0x5638ee10fc53 <unknown>\n#6 0x5638ee0e0db3 <unknown>\n#7 0x5638ee0e177e <unknown>\n#8 0x5638ee37086b <unknown>\n#9 0x5638ee374885 <unknown>\n#10 0x5638ee35e181 <unknown>\n#11 0x5638ee375412 <unknown>\n#12 0x5638ee34225f <unknown>\n#13 0x5638ee399528 <unknown>\n#14 0x5638ee399723 <unknown>\n#15 0x5638ee3aa0e4 <unknown>\n#16 0x7fd67f375609 start_thread\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m    \u001b[38;5;66;03m# Assuming the JSON is an array of values\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     scraped_pages_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(scraped_pages)\n\u001b[0;32m---> 78\u001b[0m scraper \u001b[38;5;241m=\u001b[39m \u001b[43mNormattivaAllLawsScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHROME_DRIVERS_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m scraper\u001b[38;5;241m.\u001b[39mnavigate_to_page(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.normattiva.it/ricerca/elencoPerData\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m years \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mget_years()\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mNormattivaAllLawsScraper.__init__\u001b[0;34m(self, driver_path, headless)\u001b[0m\n\u001b[1;32m      5\u001b[0m     chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--headless\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m Service(driver_path)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/chromium/webdriver.py:66\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     57\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[1;32m     58\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[1;32m     59\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:212\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authenticator_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_client()\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcapabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_websocket_connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:299\u001b[0m, in \u001b[0;36mWebDriver.start_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new session with the desired capabilities.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m:Args:\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m - capabilities - a capabilities dict to start the session with.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m caps \u001b[38;5;241m=\u001b[39m _create_caps(capabilities)\n\u001b[0;32m--> 299\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNEW_SESSION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapabilities\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: Chrome failed to start: exited normally.\n  (session not created: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /opt/google/chrome/chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x5638ee3aaf33 <unknown>\n#1 0x5638ee0a2ce6 <unknown>\n#2 0x5638ee0d76fa <unknown>\n#3 0x5638ee0d36ed <unknown>\n#4 0x5638ee11c71c <unknown>\n#5 0x5638ee10fc53 <unknown>\n#6 0x5638ee0e0db3 <unknown>\n#7 0x5638ee0e177e <unknown>\n#8 0x5638ee37086b <unknown>\n#9 0x5638ee374885 <unknown>\n#10 0x5638ee35e181 <unknown>\n#11 0x5638ee375412 <unknown>\n#12 0x5638ee34225f <unknown>\n#13 0x5638ee399528 <unknown>\n#14 0x5638ee399723 <unknown>\n#15 0x5638ee3aa0e4 <unknown>\n#16 0x7fd67f375609 start_thread\n"
     ]
    }
   ],
   "source": [
    "class NormattivaAllLawsScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    def get_years(self):\n",
    "        years = self.driver.find_elements(By.CLASS_NAME, \"btn-secondary\")\n",
    "        time.sleep(1)\n",
    "        return years\n",
    "        \n",
    "    # Get the text of a specific article\n",
    "    def get_articles(self, year, law_number):\n",
    "        print(f\"{year} - {law_number}\")\n",
    "        articles_list = []\n",
    "        \n",
    "        # Ensure is multivigente version\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "                \n",
    "        for i, article in enumerate(articles):\n",
    "            article_number = article.text.strip()\n",
    "            \n",
    "            multiplicatives = [\"bis\", \"ter\", \"quater\", \"quinquies\", \"sexies\"]\n",
    "            if (article_number == \"\") or (\"allegato\" in article_number.lower()) or ((not article_number.isdigit()) and (\"art\" not in article_number) and (\"Art\" not in article_number) and (not any(multiplicative in article_number for multiplicative in multiplicatives))):\n",
    "                print(\"Out \", article.text)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                article.click()                    \n",
    "                time.sleep(1)\n",
    "            except:                \n",
    "                continue\n",
    "            \n",
    "            text = self.driver.find_elements(By.CLASS_NAME, \"art-commi-div-akn\")\n",
    "            if not text:\n",
    "                text = self.driver.find_elements(By.CLASS_NAME, \"art-just-text-akn\")\n",
    "            if not text:\n",
    "                print(\"Error in article: \", article.text)\n",
    "                return articles_list            \n",
    "                \n",
    "            text = text[0]\n",
    "            \n",
    "            law_content = text.get_attribute('outerHTML')\n",
    "            law_content = law_content.replace(\"\\n\", \"\")\n",
    "            law_content = clearLawContent(law_content)\n",
    "                            \n",
    "            articles_list.append({ \n",
    "                \"law_source\": f\"Legge {law_number}\",\n",
    "                \"year\": year,\n",
    "                \"law_number\": article_number,\n",
    "                \"law_text\": law_content\n",
    "            })\n",
    "\n",
    "            print(articles_list[-1])\n",
    "\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    " \n",
    "# Check if the laws have already been scraped\n",
    "scraped_pages_set = set()\n",
    "df_all_laws = pd.DataFrame()\n",
    "if os.path.exists(ALL_ITALIAN_LAWS_SCRAPED_JSON):\n",
    "    scraped_pages = read_from_file(ALL_ITALIAN_LAWS_SCRAPED_JSON)\n",
    "    scraped_pages_set = set(scraped_pages)\n",
    "    \n",
    "    df_all_laws = read_df_from_csv(ALL_ITALIAN_LAWS_CSV)\n",
    "\n",
    "scraper = NormattivaAllLawsScraper(CHROME_DRIVERS_PATH, headless=True)\n",
    "scraper.navigate_to_page(\"https://www.normattiva.it/ricerca/elencoPerData\")\n",
    "years = scraper.get_years()\n",
    "data = []\n",
    "\n",
    "for year in range(2024, 1861, -1): # from 2024 to 1861\n",
    "    scraper.driver.get(\"https://www.normattiva.it/ricerca/avanzata\")\n",
    "    scraper.fill_field(\"annoProvvedimento\", year)\n",
    "    scraper.driver.find_element(By.CSS_SELECTOR, \"[type*='submit']\").click()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    validPage = True\n",
    "    curr_page = 1\n",
    "    law_urls = []\n",
    "    \n",
    "    while validPage:\n",
    "        laws = scraper.driver.find_elements(By.CSS_SELECTOR, \"[title^='Dettaglio atto']\")\n",
    "        \n",
    "        for law in laws:\n",
    "            law_url = law.get_attribute('href')\n",
    "            if law_url and \"LEGGE\" in law.text:                                \n",
    "                law_urls.append(law_url)\n",
    "        \n",
    "        # Try a new page of laws\n",
    "        curr_page += 1\n",
    "        pages_link = scraper.driver.find_elements(By.CLASS_NAME, \"page-link\")\n",
    "        validPage = False\n",
    "        for page in pages_link:\n",
    "            if page.text == str(curr_page):\n",
    "                validPage = True\n",
    "                page.click()\n",
    "                time.sleep(0.5)\n",
    "                break\n",
    "        \n",
    "    # Visit each law's detail page and scrape the articles\n",
    "    for i, url in enumerate(law_urls):\n",
    "        if f\"{year}/{i}\" in scraped_pages_set:\n",
    "            continue\n",
    "        scraper.driver.get(url)\n",
    "        \n",
    "        page_title = scraper.driver.title\n",
    "        law_number = extractArticleNumber(page_title)\n",
    "        \n",
    "        articles = scraper.get_articles(year, law_number)\n",
    "        \n",
    "        # Append individual articles to data, not the entire list\n",
    "        data.extend(articles)\n",
    "        \n",
    "        scraped_pages_set.add(f\"{year}/{i}\")\n",
    "        write_to_file(ALL_ITALIAN_LAWS_SCRAPED_JSON, json.dumps(list(scraped_pages_set)))\n",
    "        \n",
    "        # Create a temporary DataFrame for the new articles and append to the main DataFrame\n",
    "        df_tmp = pd.DataFrame(articles)  # Not `data` but `articles`\n",
    "        df_all_laws = pd.concat([df_all_laws, df_tmp], ignore_index=True)\n",
    "        \n",
    "        save_df_to_csv(df_all_laws, ALL_ITALIAN_LAWS_CSV)\n",
    "\n",
    "# df_all_laws now contains all the data\n",
    "df_all_laws.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post all Dlgs extraction -> Merge previous df with the laws.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cos = pd.read_csv(COSTITUZIONE_CSV)\n",
    "#df_cp = pd.read_csv(CODICE_PENALE_CSV)\n",
    "#df_cpp = pd.read_csv(CPP_CSV)\n",
    "df_cpa = pd.read_csv(CPA_CSV)\n",
    "df_dlgs = pd.read_csv(DLGS_CSV)\n",
    "\n",
    "df_laws = pd.read_csv(LAWS_CSV)\n",
    "\n",
    "final_df = pd.concat([df_cos, df_cpa, df_laws, df_dlgs], ignore_index=True)\n",
    "\n",
    "final_df.to_csv(LAWS_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/antonelli2/Thesis/Documents/Generated/invalid_laws.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
