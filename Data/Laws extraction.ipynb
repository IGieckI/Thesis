{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lxml.etree as ET\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isLinux = True\n",
    "default_linux_path = os.getcwd().replace(\"/Data\", \"/Documents/Downloaded\") if \"/Data\" in os.getcwd() else os.getcwd() + \"/Documents/Downloaded\"\n",
    "default_windows_path = os.getcwd().replace(\"\\\\Data\", \"\\\\Documents\\\\Downloaded\") if \"\\\\Data\" in os.getcwd() else os.getcwd() + \"\\\\Documents\\\\Downloaded\"\n",
    "default_path = default_linux_path if isLinux else default_windows_path\n",
    "\n",
    "DEFAULT_SAVE_DIR = default_path.replace(\"/Downloaded\", \"/Generated\") if isLinux else default_path.replace(\"\\\\Downloaded\", \"\\\\Generated\")\n",
    "CHROME_DRIVERS_PATH = \"/home/giacomo/chromedriver-linux64/chromedriver\" if isLinux else \"C:\\\\Users\\\\giaco\\\\Downloads\\\\chromedriver-win64\\\\chromedriver.exe\"\n",
    "# Unibo: /chromedriver-linux64/chromedriver or /home/antonelli2/chromedriver-linux64/chromedriver\n",
    "# WSL: /home/giacomo/chromedriver-linux64/chromedriver\n",
    "CODICE_PENALE_PDF = default_path + (\"/Codice penale well formatted edited.pdf\" if isLinux else \"\\\\Codice penale well formatted edited.pdf\")\n",
    "CODICE_PENALE_CSV = DEFAULT_SAVE_DIR + (\"/Codice penale well formatted edited.csv\" if isLinux else \"\\\\Codice penale well formatted edited.csv\")\n",
    "\n",
    "CPP_CSV = DEFAULT_SAVE_DIR + (\"/Codice procedura penale.csv\" if isLinux else \"\\\\Codice procedura penale.csv\")\n",
    "\n",
    "REF_MERG = DEFAULT_SAVE_DIR + ('/references_merged.csv' if isLinux else '\\\\references_merged.csv')\n",
    "INV_LAWS_JSON = DEFAULT_SAVE_DIR + ('/invalid_laws.json' if isLinux else '\\\\invalid_laws.json')\n",
    "DLGS_CSV = DEFAULT_SAVE_DIR + ('/dlgs.csv' if isLinux else '\\\\dlgs.csv')\n",
    "\n",
    "ALL_LAWS_CSV = DEFAULT_SAVE_DIR + (\"/All laws extracted.csv\" if isLinux else \"\\\\All laws extracted.csv\")\n",
    "ALL_LAWS_SCRAPED_JSON = DEFAULT_SAVE_DIR + (\"/All laws.json\" if isLinux else \"\\\\All laws.json\")\n",
    "\n",
    "# Utility functions and constants\n",
    "def write_to_file(filename, content):\n",
    "    with open(filename, 'w+') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def read_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return f.read()\n",
    "    \n",
    "def read_json_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def save_df_to_csv(df, filename):\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Different kind of text extraction from each type of file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text    \n",
    "\n",
    "def clearCommaContent(content):\n",
    "    if \"<\" not in content:\n",
    "        return content\n",
    "    \n",
    "    # Check for comma class tags\n",
    "    match =  re.search(r'<span class=\"art_text_in_comma\">(.*?)</span>', comma_content, re.DOTALL)\n",
    "    comma_content = match.group(1) if match else comma_content\n",
    "    comma_content = re.sub(r\"<div class=\\\"ins-akn\\\" eid=\\\"ins_\\d+\\\">\\(\\(\", \"\", comma_content, flags=re.DOTALL)\n",
    "    comma_content = re.sub(r\"\\)\\)</div>\", \"\", comma_content, flags=re.DOTALL)\n",
    "    comma_content = re.sub(r\"\\n\", \"\", comma_content, flags=re.DOTALL)\n",
    "    comma_content = re.sub(r\"<br>\", \"\", comma_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Check for a tags\n",
    "    aPattern = re.compile(r'<a.*?>(.*?)</a>', re.DOTALL)\n",
    "    matches = aPattern.findall(content)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            content = re.sub(r'<a.*?>.*?</a>', match, content, count=1)\n",
    "\n",
    "    # Check for span tags\n",
    "    sPattern = re.compile(r'<span.*?>(.*?)</span>', re.DOTALL)\n",
    "    matches = sPattern.findall(content)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            content = re.sub(r'<span.*?>.*?</span>', match, content, count=1)\n",
    "\n",
    "    # Check for list div tags\n",
    "    dlPattern = re.compile(r'<div class=\"pointedList-rest-akn\">(.*?)</div>', re.DOTALL)\n",
    "    matches = dlPattern.findall(content)\n",
    "    if matches:\n",
    "        for match in matches:\n",
    "            content = re.sub(r'<div class=\"pointedList-rest-akn\">.*?</div>', re.escape(match), content, count=1)\n",
    "    \n",
    "    # Delete remaining tags\n",
    "    content = re.sub(r'<.+?>', \"\", content)\n",
    "    \n",
    "    return content\n",
    "\n",
    "def extractCommaNumber(articleElement):\n",
    "    articleElement = articleElement.strip()\n",
    "    if \"art.\" in articleElement:\n",
    "        return articleElement.split(\" \")[1]\n",
    "    return articleElement\n",
    "\n",
    "def extractArticleNumber(articleElement):\n",
    "    match = re.search(r'n\\..*?(\\d+)', articleElement)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    raise Exception(\"Article number not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Codice Penale from website of Procura Generale Trento (**OUTDATED**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_articles(text):\n",
    "    # Remove all sections starting with \"LIBRO\"\n",
    "    text = re.sub(r'LIBRO.*?(?=Articolo n\\.|$)', '', text, flags=re.DOTALL)\n",
    "\n",
    "    pattern = r'(Articolo n\\..*?)(\\n.*?)(?=\\nArticolo n\\.|$)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "text = extract_text_from_pdf(CODICE_PENALE_PDF)\n",
    "\n",
    "matches = find_articles(text)\n",
    "\n",
    "data = []\n",
    "for article in matches:\n",
    "    law_number = article[0].split(\"Articolo n.\")[1] # Get everything after \"Articolo n.\"\n",
    "    law_text = article[1].strip()\n",
    "    if '.' in law_text:\n",
    "        law_title, law_text = map(str.strip, law_text.split('.', 1))\n",
    "    else:\n",
    "        law_title = ''\n",
    "    data.append({'Source': \"c.p.p.\",'Law number': law_number, 'Law title': law_title, 'Law text': law_text})\n",
    "\n",
    "df_cp = pd.DataFrame(data)\n",
    "df_cp.to_csv(CODICE_PENALE_CSV, index=False)\n",
    "\n",
    "df_cp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Codice Penale from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormattivaCpScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    # Get the text of a specific article\n",
    "    def get_cp_articles(self):\n",
    "        articles_list = []\n",
    "        \n",
    "        # Ensure is multivigente version\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        #print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for i, article in enumerate(articles[1:]):\n",
    "            if article.text.strip() != \"\" and article.text[0].isdigit():                \n",
    "                try:\n",
    "                    article.click()                    \n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    article_title = self.driver.find_element(By.CLASS_NAME, \"article-num-akn\").text.strip()\n",
    "                    commas = self.driver.find_elements(By.CLASS_NAME, \"art-comma-div-akn\")                    \n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                #print(\"Comma len: \", len(commas))\n",
    "                if len(commas) == 0:\n",
    "                    continue\n",
    "                \n",
    "                page_title =  scraper.driver.title\n",
    "                law_number = extractArticleNumber(page_title)\n",
    "                \n",
    "                firstTime = True\n",
    "                finale_text = \"\"\n",
    "                for comma in commas:\n",
    "                    if firstTime:\n",
    "                        time.sleep(1)\n",
    "                        firstTime = False\n",
    "                    \n",
    "                    # Check if the content contains any multivigenza change\n",
    "                    try:\n",
    "                        comma_content = comma.get_attribute('outerHTML')\n",
    "                    except:\n",
    "                        continue\n",
    "                    #print(comma_content)\n",
    "                    \n",
    "                    # Skip if the content contains \"<span class=\"art_text_in_comma\">((</span>\" or \"<span class=\"art_text_in_comma\">))</span>\"\n",
    "                    if \"<span class=\\\"art_text_in_comma\\\">((</span>\" in comma_content or \"<span class=\\\"art_text_in_comma\\\">))</span>\" in comma_content:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        comma_number = comma.find_element(By.CLASS_NAME, \"comma-num-akn\").text.strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    comma_content_element = comma.text\n",
    "\n",
    "                    # Clear the output\n",
    "                    comma_content = clearCommaContent(comma_content_element)\n",
    "                    finale_text += comma_content.strip()\n",
    "                articles_list.append({ \"article_source\": \"c.p.\",\n",
    "                                       \"article_text\": finale_text,\n",
    "                                       \"article_number\": law_number,\n",
    "                                       \"year\": None })\n",
    "                print(articles_list[-1])\n",
    "            else:\n",
    "                pass\n",
    "#{'article_source': 'All laws', 'article_text': \"...\", 'year': 2021, 'article_number': 242}\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "scraper = NormattivaCpScraper(CHROME_DRIVERS_PATH, headless=True)\n",
    "scraper.navigate_to_page(\"https://www.normattiva.it/uri-res/N2Ls?urn:nir:stato:regio.decreto:1930-10-19;1398\")\n",
    "articles = scraper.get_cp_articles()\n",
    "\n",
    "df_cp = pd.DataFrame(articles)\n",
    "df_cp.to_csv(CODICE_PENALE_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of C.P.P. from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormattivaCppScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    # Get the text of a specific article\n",
    "    def get_cpp_articles(self):\n",
    "        articles_list = []\n",
    "        \n",
    "        # Ensure is multivigente version\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        #print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for i, article in enumerate(articles):\n",
    "            if article.text.strip() != \"\" and article.text[0].isdigit():                \n",
    "                try:\n",
    "                    article.click()                    \n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    article_title = self.driver.find_element(By.CLASS_NAME, \"article-num-akn\").text.strip()\n",
    "                    commas = self.driver.find_elements(By.CLASS_NAME, \"art-comma-div-akn\")                    \n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                #print(\"Comma len: \", len(commas))\n",
    "                if len(commas) == 0:\n",
    "                    continue\n",
    "                \n",
    "                page_title =  scraper.driver.title\n",
    "                law_number = extractArticleNumber(page_title)\n",
    "                \n",
    "                firstTime = True\n",
    "                finale_text = \"\"\n",
    "                for comma in commas:\n",
    "                    if firstTime:\n",
    "                        time.sleep(1)\n",
    "                        firstTime = False\n",
    "                    \n",
    "                    # Check if the content contains any multivigenza change\n",
    "                    try:\n",
    "                        comma_content = comma.get_attribute('outerHTML')\n",
    "                    except:\n",
    "                        continue\n",
    "                    #print(comma_content)\n",
    "                    \n",
    "                    # Skip if the content contains \"<span class=\"art_text_in_comma\">((</span>\" or \"<span class=\"art_text_in_comma\">))</span>\"\n",
    "                    if \"<span class=\\\"art_text_in_comma\\\">((</span>\" in comma_content or \"<span class=\\\"art_text_in_comma\\\">))</span>\" in comma_content:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        comma_number = comma.find_element(By.CLASS_NAME, \"comma-num-akn\").text.strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    comma_content_element = comma.text#find_element(By.CLASS_NAME, \"art_text_in_comma\")\n",
    "\n",
    "                    # Clear the output\n",
    "                    comma_content = clearCommaContent(comma_content_element)\n",
    "                    finale_text += comma_content\n",
    "                    \n",
    "                    \n",
    "                articles_list.append({ \"article_source\": \"c.p.p.\",\n",
    "                                    \"article_text\": comma_content.strip(),\n",
    "                                    \"article_number\": law_number,\n",
    "                                    \"year\": None })\n",
    "                print(articles_list[-1])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "scraper = NormattivaCppScraper(CHROME_DRIVERS_PATH, headless=True)\n",
    "scraper.navigate_to_page(\"https://www.normattiva.it/uri-res/N2Ls?urn:nir:stato:decreto.del.presidente.della.repubblica:1988-09-22;447\")\n",
    "articles = scraper.get_cpp_articles()\n",
    "\n",
    "df_cpp = pd.DataFrame(articles)\n",
    "df_cpp.to_csv(CPP_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Dlgs from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormattivaDlgsScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    # Get the text of a specific article\n",
    "    def get_article_text(self, numeroProvvedimento, anno, article_num=[]):\n",
    "        articles_list = []\n",
    "            \n",
    "        self.fill_field(\"numeroProvvedimento\", numeroProvvedimento)\n",
    "        self.fill_field(\"annoProvvedimento\", anno)\n",
    "\n",
    "        self.driver.find_element(By.CSS_SELECTOR, \"[type*='submit']\").click()\n",
    "        self.driver.find_elements(By.CSS_SELECTOR, \"[title*='Dettaglio atto']\")[0].click()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        # Ensure is multivigente version\n",
    "        multivigente_button = self.driver.find_element(By.XPATH, '//a[contains(@href, \"multivigenza\")]')\n",
    "        multivigente_button.click()\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for article in articles:            \n",
    "            if article.text.strip() != \"\" and article.text[0].isdigit() and \"orig\" not in article.text and \"Allegato\" not in article.text and \"agg\" not in article.text:\n",
    "                print(\"In \", article.text)\n",
    "                try:\n",
    "                    article.click()\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                time.sleep(2)\n",
    "                \n",
    "                article_title = self.driver.find_element(By.CLASS_NAME, \"article-num-akn\").text.strip()\n",
    "                commas = self.driver.find_elements(By.CLASS_NAME, \"art-comma-div-akn\")\n",
    "                \n",
    "                if len(commas) == 0:\n",
    "                    continue\n",
    "                \n",
    "                page_title =  scraper.driver.title\n",
    "                law_number = extractArticleNumber(page_title)\n",
    "                \n",
    "                firstTime = True\n",
    "                finale_text = \"\"\n",
    "                for i, comma in enumerate(commas):\n",
    "                    if firstTime:\n",
    "                        time.sleep(1)\n",
    "                        firstTime = False\n",
    "                    print(numeroProvvedimento, anno, article_title, i)\n",
    "                    \n",
    "                    # Check if the content contains any multivigenza change\n",
    "                    comma_content = comma.get_attribute('outerHTML')\n",
    "                    #print(comma_content)\n",
    "                    \n",
    "                    # Skip if the content contains \"<span class=\"art_text_in_comma\">((</span>\" or \"<span class=\"art_text_in_comma\">))</span>\"\n",
    "                    if \"<span class=\\\"art_text_in_comma\\\">((</span>\" in comma_content or \"<span class=\\\"art_text_in_comma\\\">))</span>\" in comma_content:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        comma_number = comma.find_element(By.CLASS_NAME, \"comma-num-akn\").text.strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    comma_content_element = comma.text#find_element(By.CLASS_NAME, \"art_text_in_comma\")\n",
    "\n",
    "                    #print(comma_number, comma_content)\n",
    "                    \n",
    "                    # Clear the output\n",
    "                    match =  re.search(r'<span class=\"art_text_in_comma\">(.*?)</span>', comma_content, re.DOTALL)\n",
    "                    comma_content = match.group(1) if match else comma_content\n",
    "                    comma_content = re.sub(r\"<div class=\\\"ins-akn\\\" eid=\\\"ins_\\d+\\\">\\(\\(\", \"\", comma_content, flags=re.DOTALL)\n",
    "                    comma_content = re.sub(r\"\\)\\)</div>\", \"\", comma_content, flags=re.DOTALL)\n",
    "                    comma_content = re.sub(r\"\\n\", \"\", comma_content, flags=re.DOTALL)\n",
    "                    comma_content = re.sub(r\"<br>\", \"\", comma_content, flags=re.DOTALL)\n",
    "                    \n",
    "                    comma_content = clearCommaContent(comma_content_element)\n",
    "                    finale_text += comma_content.strip()\n",
    "                    \n",
    "                articles_list.append({ \"article_source\": f\"{numeroProvvedimento}/{anno}\".strip(),\n",
    "                                       \"article_text\": finale_text,\n",
    "                                       \"article_number\": law_number,\n",
    "                                       \"year\": None })\n",
    "            else:\n",
    "                print(\"Out \", article.text)\n",
    "\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "def save_articles(articles, filename):\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "scraper = NormattivaDlgsScraper(CHROME_DRIVERS_PATH, headless=True)\n",
    "\n",
    "# Check if the laws have already been scraped\n",
    "scraped_laws_set = set()\n",
    "if os.path.exists(ALL_LAWS_CSV):\n",
    "    df_scraped = pd.read_csv(ALL_LAWS_CSV)\n",
    "    first_column = df_scraped.iloc[:, 0]\n",
    "    scraped_laws = df_scraped.to_dict('records')\n",
    "    \n",
    "    # put the already scraped laws in a set to not scrape them again\n",
    "    scraped_laws_set = set(first_column)\n",
    "else:\n",
    "    scraped_laws = []\n",
    "\n",
    "invalid_laws_set = set()\n",
    "if os.path.exists(INV_LAWS_JSON):\n",
    "    invalid_laws = json.loads(INV_LAWS_JSON)\n",
    "    invalid_laws_set = set(invalid_laws)\n",
    "else:\n",
    "    invalid_laws = []\n",
    "\n",
    "scraped_laws_set.update(invalid_laws_set)\n",
    "\n",
    "df_ref = pd.read_csv(REF_MERG)\n",
    "articles = []\n",
    "\n",
    "for index, row in tqdm(df_ref.iterrows(), total=df_ref.shape[0]): # 445\n",
    "    # Check if it's a D. lgs.\n",
    "    if '/' not in row['Source'] or row['Source'].strip() in scraped_laws_set:\n",
    "        continue\n",
    "    scraped_laws_set.add(row['Source'])\n",
    "    print(f\"Scraping |{row['Source']}|\")\n",
    "    num, year = row['Source'].split(\"/\")\n",
    "            \n",
    "    scraper.navigate_to_page(\"https://www.normattiva.it/ricerca/avanzata\")\n",
    "    res = scraper.get_article_text(num, year)\n",
    "    if res == []:\n",
    "        invalid_laws.append(f\"{num}/{year}\")\n",
    "    else:\n",
    "        articles.append(res)\n",
    "    \n",
    "    if articles and len(articles) % 3 == 0:\n",
    "        save_articles([item for sublist in articles for item in sublist] + scraped_laws, ALL_LAWS_CSV)\n",
    "        with open(INV_LAWS_JSON, 'w') as f:\n",
    "            json.dumps(scraped_laws)\n",
    "\n",
    "save_articles([item for sublist in articles for item in sublist] + scraped_laws, ALL_LAWS_CSV)\n",
    "with open(INV_LAWS_JSON, 'w') as f:\n",
    "    json.dumps(scraped_laws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of all italian's laws from Normattiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormattivaAllLawsScraper:\n",
    "    def __init__(self, driver_path, headless=True):\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        self.service = Service(driver_path)\n",
    "        self.driver = webdriver.Chrome(service=self.service, options=chrome_options)\n",
    "    \n",
    "    # Get the originario version of the law\n",
    "    def fill_field(self, field_id, value):\n",
    "        input_field = self.driver.find_element(By.ID, field_id)\n",
    "        input_field.clear()\n",
    "        input_field.send_keys(value)\n",
    "    \n",
    "    def get_years(self):\n",
    "        years = self.driver.find_elements(By.CLASS_NAME, \"btn-secondary\")\n",
    "        time.sleep(1)\n",
    "        return years\n",
    "        \n",
    "    # Get the text of a specific article\n",
    "    def get_articles(self):\n",
    "        articles_list = []\n",
    "        \n",
    "        # Ensure is multivigente version\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Get articles\n",
    "        albero = self.driver.find_element(By.ID, \"albero\")\n",
    "        articles = albero.find_elements(By.CLASS_NAME, \"numero_articolo\")\n",
    "        print(\"Article len: \", len(articles))\n",
    "                \n",
    "        for i, article in enumerate(articles[1:]):            \n",
    "            print(i)\n",
    "            if article.text.strip() != \"\" and article.text[0].isdigit():                \n",
    "                try:\n",
    "                    article.click()                    \n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    article_title = self.driver.find_element(By.CLASS_NAME, \"article-num-akn\").text.strip()\n",
    "                    commas = self.driver.find_elements(By.CLASS_NAME, \"art-comma-div-akn\")                    \n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                print(\"Comma len: \", len(commas))\n",
    "                if len(commas) == 0:\n",
    "                    continue\n",
    "                \n",
    "                firstTime = True                \n",
    "                for comma in commas:\n",
    "                    if firstTime:\n",
    "                        time.sleep(1)\n",
    "                        firstTime = False\n",
    "                    \n",
    "                    # Check if the content contains any multivigenza change\n",
    "                    try:\n",
    "                        comma_content = comma.get_attribute('outerHTML')\n",
    "                    except:\n",
    "                        continue\n",
    "                    #print(comma_content)\n",
    "                    \n",
    "                    # Skip if the content contains \"<span class=\"art_text_in_comma\">((</span>\" or \"<span class=\"art_text_in_comma\">))</span>\"\n",
    "                    if \"<span class=\\\"art_text_in_comma\\\">((</span>\" in comma_content or \"<span class=\\\"art_text_in_comma\\\">))</span>\" in comma_content:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        comma_number = comma.find_element(By.CLASS_NAME, \"comma-num-akn\").text.strip()\n",
    "                    except:\n",
    "                        continue\n",
    "                    comma_content_element = comma.text#find_element(By.CLASS_NAME, \"art_text_in_comma\")\n",
    "\n",
    "                    # Clear the output\n",
    "                    comma_content = clearCommaContent(comma_content_element)\n",
    "                    \n",
    "                    print(article_title, comma_number, comma_content)\n",
    "                    \n",
    "                    articles_list.append({ \"Source\": \"All laws\",\n",
    "                                            \"Article\": article_title,\n",
    "                                            \"Comma number\": comma_number,\n",
    "                                            \"Comma content\": comma_content.strip()}) # Numeration not working in case of -bis... extract\n",
    "            else:\n",
    "                print(\"Out \", article.text)\n",
    "\n",
    "        return articles_list\n",
    "\n",
    "    # Navigate to a specific page\n",
    "    def navigate_to_page(self, url):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "    # Close the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "scraper = NormattivaAllLawsScraper(CHROME_DRIVERS_PATH, headless=False)\n",
    "scraper.navigate_to_page(\"https://www.normattiva.it/ricerca/elencoPerData\")\n",
    "years = scraper.get_years()\n",
    "data = []\n",
    "\n",
    "for year in range(1861, 2025):\n",
    "    scraper.driver.get(\"https://www.normattiva.it/ricerca/avanzata\")\n",
    "    scraper.fill_field(\"annoProvvedimento\", year)\n",
    "    scraper.driver.find_element(By.CSS_SELECTOR, \"[type*='submit']\").click()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    validPage = True\n",
    "    curr_page = 1\n",
    "    law_urls = []\n",
    "    \n",
    "    while validPage:\n",
    "        try:\n",
    "            page = year.find_element_by_xpath(\"//a[contains(text(), '{curr_page}')]\")\n",
    "            curr_page += 1\n",
    "        except:\n",
    "            validPage = False\n",
    "            continue\n",
    "        \n",
    "        laws = year.find_element_by_xpath('//a[@title=\"Dettaglio atto\"]')\n",
    "        \n",
    "        for law in laws:\n",
    "            law.click()\n",
    "            articles = scraper.get_articles()\n",
    "            \n",
    "            data.append(articles)    \n",
    "\n",
    "df_all_laws = pd.DataFrame(data)\n",
    "df_all_laws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post all Dlgs extraction -> Merge previous df with the laws.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_laws = pd.read_csv(LAWS_CSV)\n",
    "df_cp = pd.read_csv(CODICE_PENALE_CSV)\n",
    "df_cpp = pd.read_csv(CPP_CSV)\n",
    "#df_dlgs = pd.read_csv(LAWS_CSV)\n",
    "final_df = pd.concat([df_cp, df_cpp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/antonelli2/Thesis/Documents/Generated/invalid_laws.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
